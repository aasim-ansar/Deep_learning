# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8_6r3lxoioxBz-aBkXpZcxjnp5gbwmG
"""

# Install required packages
!pip install google-generativeai
!pip install langchain
!pip install langchain-google-genai
!pip install pypdf
!pip install faiss-cpu
!pip install tiktoken
!pip install langchain-community
!pip install sentence-transformers langchain

# Import necessary libraries
import google.generativeai as genai
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from operator import itemgetter

# Configure Google Gemini API
GOOGLE_API_KEY = "GOOGLE_API_KEY"
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize the Gemini LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    google_api_key=GOOGLE_API_KEY,
    temperature=0.1,
    convert_system_message_to_human=True
)

# Test the LLM
response = llm.invoke("What is EDA?")
print(response.content)

# Load and process PDF
pdf_reader = PyPDFLoader("/content/RAGPaper+(1).pdf")
documents = pdf_reader.load()
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store
db = FAISS.from_documents(documents=chunks, embedding=embeddings)

# Create prompt template
CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template("""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:""")

from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from operator import itemgetter

# Create a new prompt for answering with context
ANSWER_PROMPT = PromptTemplate.from_template(
    """You are a helpful AI assistant. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Keep the answer concise.
Question: {question}
Context: {context}
Answer:"""
)

# Define the chain for question generation
question_generator = CONDENSE_QUESTION_PROMPT | llm | str

# Define the retrieval chain
retrieval_chain = (
    {
        "context": itemgetter("standalone_question") | db.as_retriever(),
        "question": itemgetter("standalone_question"),
    }
    | ANSWER_PROMPT
    | llm
)

# Combine everything into the final conversational retrieval chain
qa = (
    RunnablePassthrough.assign(
        standalone_question=question_generator.with_config(run_name="StandaloneQuestion")
    ) | retrieval_chain
)

chat_history = []
query = """what is rag and its use case?\u200b?\u200b\u200b"""
result = qa.invoke({"question": query, "chat_history": chat_history})
print(result.content)

chat_history = []
query = """what are bert and how its related to rag?\u200b?\u200b\u200b"""
result = qa.invoke({"question": query, "chat_history": chat_history})
print(result.content)

